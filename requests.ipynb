{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# requests 蓝桥课程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.lanqiao.cn/courses/3086/learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "requests.get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36\"\n",
    "}\n",
    "url = \"https://labfile.oss.aliyuncs.com/courses/3086/lanqiao.html\"\n",
    "res = requests.get(url=url, headers=headers)\n",
    "res.encoding = 'utf-8'\n",
    "html_data = res.text\n",
    "results = re.findall('<h6 title=\"(.*)\" class=\"course-name\"', html_data)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "\n",
    "headers = {\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/84.0.4147.125 Safari/537.36\"\n",
    "}\n",
    "url = \"https://labfile.oss.aliyuncs.com/courses/3086/lanqiao.html\"\n",
    "res = requests.get(url=url, headers=headers)\n",
    "res.encoding = 'utf-8'\n",
    "html_data = res.text\n",
    "# 匹配数据，获取所有的课程标题\n",
    "results = re.findall('<h6 title=\"(.*)\" class=\"course-name\"', html_data)\n",
    "\n",
    "for item in results:\n",
    "    # 格式化字符串\n",
    "    new_str = \"课程名：{item}\\n\".format(item=item)\n",
    "    # 将格式化之后的数据写入文件\n",
    "    with open(\"./data.txt\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(new_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"http://app.mi.com/\")\n",
    "# 输出请求到的响应状态码\n",
    "print(res.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"http://app.mi.com/\")\n",
    "# 输出请求到的响应状态码\n",
    "if res.status_code == requests.codes.ok:\n",
    "    print(\"请求成功\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "requests 库响应体\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 requests 中包含如下几种获取响应内容的方法：\n",
    "\n",
    "- r.text 以文本的方式访问响应体\n",
    "- r.content 以字节的方式访问响应体\n",
    "- r.json() 将响应体直接进行 json 序列化操作\n",
    "- r.raw 获取来自服务器的原始套接字响应（注意本部分内容不再本系列实验讲授范围）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以文本的方式访问响应体\n",
    "直接获取 requests 请求发出后，服务器返回的文本数据是爬虫程序最常使用的技术，该方式一般用于网页源码的直接读取，例如下述代码，你将直接获取到百度服务器返回的网站源码。\n",
    "\n",
    "将如下代码写入 /home/project/index.py 文件中：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"http://www.baidu.com/\")\n",
    "\n",
    "if res.status_code == requests.codes.ok:\n",
    "    print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"https://www.baidu.com/\")\n",
    "# 设置响应体编码\n",
    "res.encoding = \"utf-8\"\n",
    "\n",
    "if res.status_code == requests.codes.ok:\n",
    "    print(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "img_url = (\"http://t5.market.xiaomi.com/thumbnail/jpeg/l750/AppStore\"\n",
    "           \"/08636408ad50fc7396a2dcac92ac1b18b6842ddbc\")\n",
    "res = requests.get(img_url)\n",
    "\n",
    "if res.status_code == requests.codes.ok:\n",
    "    print(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1464.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_item():\n",
    "    url = \"http://app.mi.com/topList?page=1\"\n",
    "    res = requests.get(url=url, headers=headers)\n",
    "    # 比对返回的状态码\n",
    "    if res.status_code == requests.codes.ok:\n",
    "        html = res.text\n",
    "        html = re.sub(\">\\s*<\",\"><\",html)\n",
    "\n",
    "        # 编译正则表达式，生成一个正则表达式（ Pattern ）对象\n",
    "        pattern = re.compile(\n",
    "            r'<li><a href=\"(.*?)\"><img data-src=\"(.*?)\" src=\"(.*?)\" alt=\"(.*?)\" width=\"72\" height=\"72\"></a><h5><a href=\"(.*?)\">啫喱</a></h5><p class=\"app-desc\"><a href=\"(.*?)\">聊天社交</a></p></li>')\n",
    "        m = pattern.search(html)\n",
    "        # m.group(0) 返回整个匹配的字符串\n",
    "        print(m.group(0))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第 1 个为 re.compile 方法，该方法将一个正则表达式编译成一个对象，一般用变量 pattern 接收，之后即可通过 pattern 调用 search，findall 等方法，相当于编译一次正则表达式，在多次使用编译好的对象。\n",
    "\n",
    "第 2 个知识点是 re.compile 方法中的字符串前面有个前缀 r，该符号防止字符转义，假设路径中出现 \\n ， 不加 r 的话 \\n 就会被转义，而加了 r 之后 \\n 能保留原有的样子进行输出。\n",
    "\n",
    "第 3 个知识点是 m.group(0) 将返回匹配成功的整个子字符串。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import re\n",
    "import time\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 6.2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0.1464.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def save_img(img_url, name):\n",
    "    img_res = requests.get(url=img_url, headers=headers)\n",
    "    # 比对返回的状态码\n",
    "    if img_res.status_code == requests.codes.ok:\n",
    "        data = img_res.content\n",
    "        with open(\"./icons/{name}.png\".format(name=name), \"wb\") as f:\n",
    "            f.write(data)\n",
    "        print(\"{name} - 图片存储完毕\".format(name=name))\n",
    "\n",
    "def get_item(page):\n",
    "    url = \"http://app.mi.com/topList?page={page}\".format(page=page)\n",
    "    res = requests.get(url=url, headers=headers)\n",
    "    # 比对返回的状态码\n",
    "    if res.status_code == requests.codes.ok:\n",
    "        html = res.text\n",
    "        html = re.sub(\">\\s*<\",\"><\",html)\n",
    "        # 编译正则表达式，生成一个正则表达式（ Pattern ）对象\n",
    "        pattern = re.compile(\n",
    "            r'<li><a href=\"/details.*?\"><img data-src=\"(.*?)\" src=\".*?\" alt=\".*?\" width=\".*?\" height=\".*?\"></a><h5><a href=\".*?\">(.*?)</a></h5><p class=\".*?\"><a href=\".*?\">(.*?)</a></p></li>')\n",
    "        items = pattern.findall(html)\n",
    "        # m.group(0) 返回整个匹配的字符串\n",
    "        # print(items)\n",
    "        for item in items:\n",
    "            # item 数据为 ('图片地址', '我的世界夏季版', '休闲创意')\n",
    "            save_img(item[0], item[1])\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for page in range(1, 43):\n",
    "        print(\"正在爬取第{page}页\".format(page=page))\n",
    "        get_item(page)\n",
    "        time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lxml 库与 cssselect 库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def get_pokemons():\n",
    "    res = requests.get(\n",
    "        \"https://labfile.oss.aliyuncs.com/courses/3086/target.html\", headers=headers)\n",
    "    res.encoding = 'utf-8'\n",
    "    data = res.text\n",
    "    with open(\"./temp/target.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(data)\n",
    "    print(data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_pokemons()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试使用本地文件读取有没有问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import html\n",
    "\n",
    "# 从网络获取的图片，本实验中表示从本地存储的 HTML 读取的数据\n",
    "def get_pokemons_fromfile():\n",
    "    try:\n",
    "        with open(\"./temp/target.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "            data = f.read()\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def analysis(data):\n",
    "    # 将网页源码格式化成 element 对象\n",
    "    page_tree = html.fromstring(data)\n",
    "    print(page_tree)\n",
    "    print(type(page_tree))\n",
    "    # 通过 CSS 选择器获取 所有 title 标签\n",
    "    eles = page_tree.cssselect('title')\n",
    "    print(eles)\n",
    "    print(type(eles))\n",
    "    # 获取 title 标签内文本\n",
    "    print(eles[0].text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = get_pokemons_fromfile()\n",
    "    analysis(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上述代码请注意先导入 lxml 模块下面的 html 对象，然后通过 html 对象的 fromstring 方法将网页源码实例化为 element 对象，在对实例化的 element 对象调用 cssselect 方法，该方法的参数为 CSS 选择器。（此处需要对 CSS 选择器基础知识熟悉，可以在 W3School 学习）,还要注意 page_tree.cssselect('title') 表示选择网页源码中的所有 title 标签，获取到的是一个 list 对象，所以在获取标签内部文本时需要通过 eles[0].text 获取。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from lxml import etree\n",
    "\n",
    "def get_pokemons_fromfile():\n",
    "    try:\n",
    "        with open(\"./temp/target.html\", \"r\", encoding=\"utf-8\") as f:\n",
    "            data = f.read()\n",
    "            return data\n",
    "    except Exception as e:\n",
    "        return None\n",
    "\n",
    "def analysis_byxpath(data):\n",
    "    html = etree.HTML(data)\n",
    "    print(html)\n",
    "    # 选取页面中 title 标签\n",
    "    title = html.xpath(\"//title\")\n",
    "    print(title)\n",
    "    # 选取页面中所有的 a 标签\n",
    "    all_a = html.xpath(\"//a\")\n",
    "    # 输出所有 a 标签的文本\n",
    "    all_a_text = html.xpath(\"//a/text()\")\n",
    "    print(all_a_text)\n",
    "    # 输出 id=\"firstHeading\" 的 h1 标签的文本内容\n",
    "    h1_text = html.xpath(\"//h1[@id='firstHeading']/text()\")\n",
    "    print(h1_text)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    data = get_pokemons_fromfile()\n",
    "    analysis_byxpath(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特别注意 xpath 方法中的参数 //title 表示从根目录匹配所有的 title 标签。其中的 // 的含义需要说明一下，在很多教材中该内容说的都有点绕，你可以将其理解成在当前网页文档中进行全局检索，只要找到名字叫做 title 的标签就能匹配到（本例子用到的是 title，你修改成 a，就是在网页文档中全局检索 a 标签）。\n",
    "\n",
    "与 // 对应的是 / 该符号表示从根结点开始查找，相同的写法例如在下述 HTML 代码中寻找 title，可以看一下两个写法的区别，先提供一段待解析 HTML 代码：\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` html\n",
    "<html>\n",
    "  <head>\n",
    "    <title>这是橡皮擦的测试网页</title>\n",
    "  </head>\n",
    "  <body>\n",
    "    <h1>标题内容</h1>\n",
    "  </body>\n",
    "</html>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "匹配 title，// 的写法是 //title ，而 / 的写法是从根结点 html 开始查找，故写法为 /html/head/title，甚至你可以写成 /html//title，它的含义是现在根结点开始找到 html 标签，再在 html 标签中全局检索 title（有前端基础的同学可以配合后代标签学习）。以上内容展开讲解为绝对路径与相对路径问题，本系列实验以应用为主，不在细挖差异化内容，有兴趣的同学在掌握基本用法之后，可以通过搜索引擎补充相关知识。\n",
    "\n",
    "继续查看上述 Python 代码，在获取 a 标签所有文字的代码段中，通过 //a/text() 可以获取 a 标签中的文本，核心代码为 text()，获取双标签中的文本都采用该方式即可，例如获取网页标题，可以通过 //title/text() 获取。\n",
    "\n",
    "接下来要说明的是 @，在上述代码中应用的部分为 //h1[@id='firstHeading']/text()，该内容表示选择 h1 标签，但前提是该 h1 标签的 id 属性要等于 firstHeading，这里就涉及了 @ 的用法，该符号表示选取属性，即 HTML 标签属性。选择 class 属性可以用 @class，选择 href 属性用 @href。如果选择特定携带某些属性的标签，使用 标签名[@属性名=属性值] 即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xpath语法初步总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XPath 语法初步总结\n",
    "应用 lxml 的 XPath 语法需要首先导入对象，之后调用对象的 HTML 方法，实例化为 Element 对象。\n",
    "```python\n",
    "from lxml import etree\n",
    "html = etree.HTML(data)\n",
    "copy\n",
    "1. 获取指定标签：\n",
    "\n",
    "# 指定标签获取：\n",
    "html.xpath('//标签名称')\n",
    "html.xpath('/标签名称/标签名称')\n",
    "```\n",
    "2. 获取标签文本\n",
    "\n",
    "# 标签文本获取：\n",
    "html.xpath('//标签名称/text()')\n",
    "html.xpath('/标签名称/标签名称/text()')\n",
    "```\n",
    "3. 匹配携带属性的标签\n",
    "\n",
    "# 只要携带某属性就匹配\n",
    "html.xpath('//标签名称[@某属性]')\n",
    "# 携带某属性并且该属性等于某个值\n",
    "html.xpath('//标签名称[@某属性=某个值]')\n",
    "```\n",
    "对于 XPath，本实验只揭开了冰山一角，更多的内容在后续实验中会逐步展开，如果等不及可以提前打开 菜鸟教程 进行预习，对后续实验学习有非常大的帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其余代码在上文中已经存在，本部分只展示差异部分\n",
    "with open('./temp/target.html','r',encoding=\"utf-8\") as f:\n",
    "    re = f.read()\n",
    "    \n",
    "data = re\n",
    "# print(data)\n",
    "def analysis_byxpath(data):\n",
    "    html = etree.HTML(data)\n",
    "    all_li = html.xpath(\"//div[@class='mf-section-1']//li\")\n",
    "    for li in all_li:\n",
    "        pid = li.xpath('./text()')[1]\n",
    "        name = li.xpath('./a/text()')[0]\n",
    "        print(pid,name)\n",
    "    print(all_li)\n",
    "\n",
    "analysis_byxpath(data=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# beautiful soup3解析库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36 Edg/86.0.622.69'\n",
    "}\n",
    "\n",
    "def get_detail():\n",
    "    r = requests.get(\"https://labfile.oss.aliyuncs.com/courses/3086/books.html\", headers=headers)\n",
    "    r.encoding = 'utf-8'\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "    title_tag = soup.title\n",
    "\n",
    "    title_str = title_tag.string\n",
    "\n",
    "    # Tag 标签对象\n",
    "    print(type(title_tag))\n",
    "\n",
    "    # NavigableString 对象为字符串内容\n",
    "    print(type(title_str))\n",
    "\n",
    "    # BeautifulSoup 对象为文档的全部内容\n",
    "    print(type(soup))\n",
    "\n",
    "    # Comment 对象为文档注释内容\n",
    "    # print(type(comment))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_detail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_detail():\n",
    "    r = requests.get(\"https://labfile.oss.aliyuncs.com/courses/3086/books.html\", headers=headers)\n",
    "    r.encoding = 'utf-8'\n",
    "    html_doc = r.text\n",
    "    soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "    # 获取 title 标签\n",
    "    title_tag = soup.title\n",
    "    print(title_tag)\n",
    "    # 获取 head 标签\n",
    "    head_tag = soup.head\n",
    "    print(head_tag)\n",
    "    # 获取 tag 标签\n",
    "    a_tag = soup.a\n",
    "    print(a_tag)\n",
    "    # 获取 p 标签\n",
    "    p_tag = soup.p\n",
    "    print(p_tag)\n",
    "    a_tag = soup.a\n",
    "    print(a_tag.attrs)\n",
    "    # 通过 attrs 再获取\n",
    "    print(a_tag.attrs[\"href\"])\n",
    "    # 直接获取 a 标签的 class 属性\n",
    "    print(a_tag[\"class\"])\n",
    "get_detail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "方法选择器\n",
    "通过标签选择器选择网页元素整体速度快，但是并不灵活，所以在很多时候需要借助方法选择器（也称做文档树搜索方法）对网页元素进行查找。\n",
    "\n",
    "首先要介绍的是最常用的两个方法，find_all 与 find 方法。\n",
    "\n",
    "find_all 的语法格式如下：\n",
    "\n",
    "find_all(name, attrs, recursive, text, **kwargs)\n",
    "```\n",
    "其中各参数说明如下：\n",
    "\n",
    "name，可以是标签名称，正则表达式，列表，自定义函数等内容；\n",
    "attrs，配合前面的标签增加属性过滤，可传递多属性；\n",
    "recursive，调用 Tag 的 find_all 方法时，bs4 会检索当前 Tag 的所有子孙节点，如果只想搜索 Tag 的直接子节点，可以使用参数 recursive = False ；\n",
    "text，通过文本检索，通过 text 参数可以检索文档中的字符串内容，与 name 参数一样, text 参数接受标签名 , 正则表达式 , 列表等内容；\n",
    "keyword，关键字参数，理解成按自定义属性检索即可。\n",
    "以上是对 find_all 方法的参数说明，方法选择器包含很多内容，例如以下方法，它们的使用都与 find_all 基本一致，可以自行学习掌握。\n",
    "\n",
    "find_all 与 find\n",
    "find_parents 与 find_parent\n",
    "find_next_siblings 与 find_next_sibling\n",
    "find_previous_siblings 与 find_previous_sibling\n",
    "find_all_next 与 find_next\n",
    "find_all_previous 与 find_previous\n",
    "接下来通过代码实际查看一下 find_all 方法的应用。find 方法语法格式与 find_all 一致，只是返回的是单一标签，不再赘述。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSS 选择器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CSS 选择器\n",
    "该选择器与 find_all 方法非常相似，通过一个 select 方法进行网页标签选择，学习该内容依旧需要对前端基础中的 HTML+CSS 有所了解。\n",
    "\n",
    "通过标签名进行查找\n",
    "\n",
    "该方式起始就是在使用 CSS 中的选择器，代码如下：\n",
    "```python\n",
    "title_tag = soup.select(\"title\")\n",
    "print(title_tag)\n",
    "```\n",
    "需要特别注意的是 select 方法返回的数据类型是列表。接下来的知识内容将配合代码学习，在时间允许的情况下，尽量临摹几遍代码，做到牢牢掌握该内容，知识点不在进行文字特别说明，具体查找方式如下。\n",
    "\n",
    "通过标签 class 属性（类名）进行查找\n",
    "```python\n",
    "div_tag = soup.select(\".jl-book-item\")\n",
    "print(div_tag)\n",
    "```\n",
    "通过 id 查找\n",
    "```python\n",
    "div_tag = soup.select(\"#app\")\n",
    "print(div_tag)\n",
    "```\n",
    "通过任意属性查找\n",
    "```python\n",
    "div_tags = soup.select(\"div[class='jl-book-item']\")\n",
    "print(div_tags)\n",
    "```\n",
    "直接子标签查找\n",
    "```python\n",
    "# 注意 a 标签前的 >\n",
    "a_tags = soup.select(\"div[class='jl-book-item'] > a\")\n",
    "print(a_tags)\n",
    "```\n",
    "后代标签查找（很多地方叫做组合查找）\n",
    "```python\n",
    "# 注意 a 标签前的 空格\n",
    "a_tags = soup.select(\"div[class='jl-book-item'] a\")\n",
    "print(a_tags)\n",
    "```\n",
    "获取内容\n",
    "select 方法获取到网页标签都是列表对象，通过遍历形式输出，然后用 get_text 方法来获取标签的内容。\n",
    "```python\n",
    "h3_tags = soup.select(\"div[class='jl-book-item'] h3\")\n",
    "for h3 in h3_tags:\n",
    "    print(h3.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试css选择器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36 Edg/86.0.622.69'\n",
    "}\n",
    "\n",
    "r = requests.get(\"https://labfile.oss.aliyuncs.com/courses/3086/books.html\", headers=headers)\n",
    "r.encoding = 'utf-8'\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "title_tag = soup.title\n",
    "\n",
    "title_str = title_tag.string\n",
    "\n",
    "# Tag 标签对象\n",
    "print(type(title_tag))\n",
    "\n",
    "# NavigableString 对象为字符串内容\n",
    "print(type(title_str))\n",
    "\n",
    "# BeautifulSoup 对象为文档的全部内容\n",
    "print(type(soup))\n",
    "\n",
    "# Comment 对象为文档注释内容\n",
    "# print(type(comment))\n",
    "\n",
    "r = requests.get(\"https://labfile.oss.aliyuncs.com/courses/3086/books.html\", headers=headers)\n",
    "r.encoding = 'utf-8'\n",
    "html_doc = r.text\n",
    "soup = BeautifulSoup(html_doc, \"lxml\")\n",
    "\n",
    "print(type(soup))\n",
    "\n",
    "'(# 标签查找)'\n",
    "h2_tags = soup.find_all('h2')\n",
    "# print(h2_tags)\n",
    "print(type(h2_tags))\n",
    "print(\"------\")\n",
    "\n",
    "# 属性加标签过滤\n",
    "div_tags = soup.find_all('div', attrs={'class': 'jl-book-item'})\n",
    "print(div_tags)\n",
    "# print(div_tags.get_text())\n",
    "# print(type(div_tags))\n",
    "print(\"------\")\n",
    "\n",
    "# 标签列表\n",
    "tag_list = soup.find_all([\"h2\", \"h3\"])\n",
    "print(tag_list)\n",
    "# print(type(tag_list))\n",
    "print(\"------\")\n",
    "\n",
    "\n",
    "print(\"------\")\n",
    "\n",
    "h3_tags = soup.select(\"div[class='jl-book-item'] h3\")\n",
    "for h3 in h3_tags:\n",
    "    print(h3.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = soup.select(\".u-clearfix\")\n",
    "d1 = soup.select(\"li[class ='menu-item menu-item-has-children menu-item--new']\")\n",
    "d1 = soup.select(\"div[id ='app'] \")\n",
    "d1 = d1.find(\"a\")\n",
    "d1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 接口（API） 爬取知识"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同步与异步。\n",
    "\n",
    "同步，服务器将网页结构与数据一起返回，呈现在你面前的网页是服务器直接打包返回的；\n",
    "异步，服务器先将网页结构返回，再通过调用接口或者其它方式将数据传输到浏览器，之后替换到网页指定位置，简单理解就是呈现在你面前的网页先呈现的是一个框架，再填的数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_data(page):\n",
    "    headers = {\n",
    "        \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.121 Safari/537.36\"\n",
    "    }\n",
    "    # 请求数据\n",
    "    payload = {'ac': 'recipe',\n",
    "               'op': 'getMoreDiffStateRecipeList',\n",
    "               'classid': '0',\n",
    "               'orderby': 'hot',\n",
    "               'page': page}\n",
    "    url = \"https://home.meishichina.com/ajax/ajax.php\"\n",
    "    res = requests.get(url, headers=headers, params=payload)\n",
    "    # 直接以 JSON 格式返回\n",
    "    print(res.text)\n",
    "    print(res.json())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_data(2)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 利用移动端web来爬取目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_data(page_num=1, page_size=20):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1\"\n",
    "    }\n",
    "    # URL 参数，其中部分为固定值\n",
    "    data = {\n",
    "        \"cId\": 2,\n",
    "        \"devicesupport\": 1,\n",
    "        \"page_size\": page_size,\n",
    "        \"page_num\": page_num,\n",
    "        \"dataTraceId\": 880\n",
    "    }\n",
    "    url = \"http://gprp.4399.com/cg/online/get_h5_ranking.php\"\n",
    "    res = requests.get(url,\n",
    "                       params=data, headers=headers)\n",
    "\n",
    "    # 直接以 JSON 格式返回\n",
    "    print(res.json())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 直接调用\n",
    "    get_data()\n",
    "    # 循环调用\n",
    "    for i in range(1, 9):\n",
    "        print(\"正在抓取第{i}页数据\".format(i=i))\n",
    "        get_data(page_num=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 注意有的参数是通过上一次返回的值决定的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "# 声明抓取函数，timeout 参数初始值需要手动获取，即第一次获取到的初始链接中的值，本案例中对应上文的\n",
    "def get_items(timeout=1607519513):\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/13.0.3 Mobile/15E148 Safari/604.1\"\n",
    "    }\n",
    "\n",
    "    url = \"https://m.smzdm.com/search/ajax_search_list\"\n",
    "    data = {\n",
    "        \"type\": \"tag\",\n",
    "        \"channel\": \"faxian\",\n",
    "        \"search_key\": \"白菜党\",\n",
    "        \"timesort\": timeout\n",
    "    }\n",
    "    # 通过 requests 模块获取网页数据\n",
    "    res = requests.get(url,\n",
    "                       params=data, headers=headers)\n",
    "    # 对获取到的 JSON 格式数据进行判断，如果包含数据，存入本地文件，否则直接结束\n",
    "    json_data = res.json()\n",
    "    # 获取所有商品\n",
    "    goods = json_data[\"data\"]\n",
    "    if len(goods) > 0:\n",
    "        print(\"商品总数为：\", len(goods))\n",
    "        # 该部分可以调用存储函数，存储成 CSV 文件\n",
    "        print(goods)\n",
    "\n",
    "        # 获取 JSON 数据中最后一条数据的关键参数，本案例中为 timeout\n",
    "        last_data = goods[-1]\n",
    "        # print(last_data)\n",
    "        timeout = last_data[\"time_sort\"]\n",
    "        # 重新调用 get_items 方法，传入新的 timeout 值\n",
    "        print(timeout)\n",
    "        time.sleep(2)\n",
    "        get_items(timeout)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    get_items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
